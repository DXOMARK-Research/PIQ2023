{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b1557dc-dfe3-4157-8aa5-f6e3cee6714e",
   "metadata": {},
   "source": [
    "# NTIRE 2024 - Portrait Quality Assessment Challenge ft. DXOMARK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9c177-e6f5-4bc1-84f1-0803b294608b",
   "metadata": {},
   "source": [
    "# -- INSTRUCTIONS --\n",
    "# **[READ CAREFULLY]**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a1d630-73c8-4231-a1bc-0caa04e8ce3f",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# 1 - DESCRIPTION\n",
    "## What is this?\n",
    "This is a simple instruction notebook to help you submit your model.\n",
    "\n",
    "## Links\n",
    "- **NTIRE 24 challenge**: https://codalab.lisn.upsaclay.fr/competitions/17311#participate\n",
    "- **PIQ23 github**: https://github.com/DXOMARK-Research/PIQ2023\n",
    "- **PIQ23 Download link**: https://corp.dxomark.com/data-base-piq23/\n",
    "\n",
    "## Test set\n",
    "- The evaluation process consists of testing your model on an internat portrait dataset of around ***200 scenes of 7 images each, each shot with a different device of close quality***.\n",
    "- Images are either jpeg or TIFF with extensions ***'.jpg' or '.tiff'***.\n",
    "- Images are either landscape or portrait with a size of: ***1280x960*** or ***960x1280***.\n",
    "- Lighting conditions are: ***Indoor, Outdoor, Lowlight and Night***.\n",
    "- Different skintones, genders and ages are used. Prepare for something slightly different than PIQ23.\n",
    "- Do not excpect the same people to be present in the internal dataset.\n",
    "- The test csv will include ***image names and the categories of each class alongside the lighting conditions***. *Please refer to the images.csv*\n",
    "\n",
    "## Hardware requirements\n",
    "- You are free to do inference on one or multiple images.\n",
    "- Please make sure that your model is able to run on a ***single 8GB VRAM GPU***.\n",
    "- Maximum Teraflops: ***5TFLOPS*** *(Refer below to calculate teraflops on your model)*.\n",
    "- Maximum inference time: ***5 seconds/image***. *Model Loading does not count*.\n",
    "- Maximum model.pth size: ***2GB***.\n",
    "- Maximum RAM: **16GB**.\n",
    "- **NOTE: If your model comply with the 16GB RAM and 5s/image on cpu, you don't need to use GPU**\n",
    "\n",
    "## Submission\n",
    "- You need to submit a script with the following naming: ***model_[LAST_NAME]_[FIRST_NAME].py***\n",
    "- Your script will be put in the ***./models/ folder***.\n",
    "- Your model weights should be saved in ***./weights/weights_[LAST_NAME]_[FIRST_NAME].(pth,ckpt)***\n",
    "- Your assets will be in ***./assets/assets_[LAST_NAME]_[FIRST_NAME]***.\n",
    "- You will get access to one level above the models/ folder.\n",
    "- Your script needs to load the ***./images.csv***.\n",
    "- Images paths are structured as follows: ***images/[class]/[imagename.(jpg,tiff)]***.\n",
    "- You need to save your results as follows: ***./results/result_[LAST_NAME]_[FIRST_NAME].csv***.\n",
    "- You need to add a ***column 'SCORE' to the images.csv***. *KEEP ALL OTHER METADATA*.\n",
    "- You can use a ***comma or semi-colon separator for the results***. Any other separator will not be considered.\n",
    "- Refer to *./models/model_23_PIQ.py* for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95a456b-49c9-448c-9583-92918450e0a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2 - ASSETS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac33bf-779f-4c11-a891-430a32129e3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0ed13e7-49cf-42e8-b20c-86298665eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTENSIONS = ('.tiff', '.tif', '.TIFF', '.TIF', '.jpg', '.JPG', '.jpeg')\n",
    "CONDITIONS = ('OUTDOOR', 'INDOOR', 'LOWLIGHT', 'NIGHT')\n",
    "IMAGE_SIZE = ((1280, 960), (960, 1280))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865a1ae0-04b3-475b-8937-e71ee935ae61",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hardware Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d8dd3a-b854-421d-9f01-92aefa04d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from thop import profile # pip install thop\n",
    "\n",
    "def torch_cuda_memory_usage():\n",
    "    \"\"\"Returns CUDA memory usage if available\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()  # Wait for all CUDA kernels to finish\n",
    "        allocated_memory = torch.cuda.memory_allocated()  # Total allocated memory\n",
    "        cached_memory = torch.cuda.memory_reserved()  # Total cached memory\n",
    "        return allocated_memory / (1024**3), cached_memory / (1024**3)  # Convert bytes to GB\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "def test_model_resources(model, batch):\n",
    "    \n",
    "    macs, params = profile(model, inputs=(batch, ), verbose=False)\n",
    "    flops = macs * 2  # Convert MACs to FLOPs\n",
    "    tflops = flops / (10**12)  # Convert FLOPs to TFLOPs  \n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()  # Reset peak memory stats for accurate peak measurement\n",
    "\n",
    "    # Measure memory before inference\n",
    "    allocated_before, cached_before = torch_cuda_memory_usage()\n",
    "    \n",
    "    model = model.cuda()  # Move model to GPU\n",
    "    batch = batch.cuda()  # Move data to GPU\n",
    "    \n",
    "    # Dummy forward pass to measure VRAM usage\n",
    "    with torch.no_grad():\n",
    "        _ = model(batch)\n",
    "        \n",
    "    # Measure memory after inference\n",
    "    allocated_after, cached_after = torch_cuda_memory_usage()\n",
    "    peak_allocated = torch.cuda.max_memory_allocated() / (1024**3)  # Peak allocated memory during inference\n",
    "    \n",
    "    vram_usage_allocated = allocated_after - allocated_before  # Approximation of additional VRAM used during inference\n",
    "    vram_usage_cached = cached_after - cached_before  # Approximation based on cached memory\n",
    "\n",
    "    print(f\"MACs: {macs}\")\n",
    "    print(f\"FLOPs: {flops}\")\n",
    "    print(f\"TFLOPs: {tflops}\")\n",
    "    print(f\"Approx. Additional VRAM Usage (Allocated) during Inference: {vram_usage_allocated} GB\")\n",
    "    print(f\"Approx. Additional VRAM Usage (Cached) during Inference: {vram_usage_cached} GB\")\n",
    "    print(f\"Peak VRAM Usage during Inference: {peak_allocated} GB\")\n",
    "    \n",
    "    del model, batch  # Free up memory\n",
    "    torch.cuda.empty_cache()  # Clear cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f28821c-e496-4e31-bf44-b7ceeb0d6641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACs: 1821354430464.0\n",
      "FLOPs: 3642708860928.0\n",
      "TFLOPs: 3.642708860928\n",
      "Approx. Additional VRAM Usage (Allocated) during Inference: 0.3515634536743164 GB\n",
      "Approx. Additional VRAM Usage (Cached) during Inference: 5.64453125 GB\n",
      "Peak VRAM Usage during Inference: 5.28789758682251 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1) # change to your model\n",
    "batch_size = 18 # Test the batch size you want\n",
    "batch = torch.stack([torch.randn(3, 1280, 960)]*batch_size)\n",
    "\n",
    "test_model_resources(model, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee76f21-6440-4eff-8294-b17711ff4e40",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3 - SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "032f8823-38b7-4201-b5d5-1aaa7aaab549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am an asset.\n"
     ]
    }
   ],
   "source": [
    "%run ./models/model_23_PIQ.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293c6276-95f6-4e08-8589-0217107d3486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
